{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {
    "id": "2f75fd94"
   },
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {
    "id": "66dcde05"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from pandas import read_parquet\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import warnings\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {
    "id": "3f1242c4"
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "SHP_FILE = 'taxi_zones.shp'\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {
    "id": "26ad10ea"
   },
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "For part 1, we mainly need to process the data according to the following steps:\n",
    "1. Define a function that can calculate the distance between two locations.\n",
    "2. Download and clean the yellow taxi data from the NYC Taxi & Limousine Commission\n",
    "3. Load and clean uber data\n",
    "4. Load and clean weather\n",
    "\n",
    "Note: The time range of yellow taxi, uber and weather is: 2009-01 to 2015-06."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {
    "id": "32074561"
   },
   "source": [
    "### Calculating distance\n",
    "Calculate the distance bewteen two locations by Haversine Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {
    "id": "4cbbe6cc"
   },
   "outputs": [],
   "source": [
    "def calculate_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "      Calculate the distance between two locations by their longitude and latitude in kilometers by Haversine formula. \n",
    "\n",
    "      Parameter Explanation in this function:\n",
    "      lon1 ----- longitude of pickup location\n",
    "      lat1 ----- latitude of pickup location\n",
    "      lon2 ----- longitude of dropoff location\n",
    "      lat2 ----- latitude of dropoff location  \n",
    "    \"\"\"\n",
    "    #Convert latitude and longitude to radius\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1\n",
    "    # Calculate the distance\n",
    "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
    "\n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
    "    #6367 for distance in KM for miles use 3958\n",
    "    km = 6367 * dist \n",
    "    return km\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {
    "id": "6d6abf52"
   },
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    \"\"\"\n",
    "    Add the series of value calculated by the calculate_distance function to the dataframe column\n",
    "\n",
    "    Parameter Explanation:\n",
    "    dataframe ---- a dataframe needs to add distance column\n",
    "\n",
    "    \"\"\"\n",
    "    dataframe['distance']=calculate_distance(dataframe['pickup_longitude'],dataframe['pickup_latitude'],dataframe['dropoff_longitude'],dataframe['dropoff_latitude'])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {
    "id": "93daa717"
   },
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "Get the urls to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {
    "id": "cbd0d198"
   },
   "outputs": [],
   "source": [
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_csv_urls():\n",
    "    # After getting the html, we are going to analyze the html and get the information we want, which is the url linked to \"Yellow Taxi Trip Records\"\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        # use regular expression to find the links that match the pattern 2009-01~2015-06\n",
    "        if re.search(r\"yellow_tripdata_2009|yellow_tripdata_201[0-4]|yellow_tripdata_2015-0[1-6]\", link.get(\"href\")):\n",
    "            urls.append(link.get(\"href\"))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xA-chqwPIuhN",
   "metadata": {
    "id": "xA-chqwPIuhN"
   },
   "source": [
    "Clean the yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {
    "id": "2f40130a"
   },
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    # the input 'url' is one of the urls we get through function 'find_taxi_csv_urls()'\n",
    "    # input: a dataframe; the url where we can download one month' taxi trip record\n",
    "    # todo: download; clean data\n",
    "    # output: a dataframe; processed trip records for one month\n",
    "    \n",
    "    \n",
    "    # download and read trip record data\n",
    "    file_name = url.split('/')[-1]\n",
    "    # download the file if it doesn't exist\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"downloading\", file_name)\n",
    "        file = requests.get(url)\n",
    "        with open(file_name , \"wb\") as f:\n",
    "            f.write(file.content)\n",
    "    trip_data = pd.read_parquet(file_name)\n",
    "    print(\"cleaning\", file_name)\n",
    "    \n",
    "\n",
    "    # scale down sample volume to match the volume of uber data\n",
    "    # samples from each month = volume of uber data / numbers of monthly taxi data packs = 2*10^5 / (6.5*12) = 2564\n",
    "    trip_data = trip_data.sample(n=2564)\n",
    "    \n",
    "    \n",
    "    # from location ID to longtitude and latitude\n",
    "    def ID_to_LonLat(df):\n",
    "        # import shp files, containing location ID info\n",
    "        \n",
    "        sf_path = r'taxi_zones.shp'\n",
    "        taxi_zone = geopandas.GeoDataFrame.from_file(sf_path)\n",
    "        # we translate the location ID into lontitude and latitude\n",
    "        taxi_zone = taxi_zone.to_crs(4326)\n",
    "        taxi_zone['lon'] = taxi_zone.centroid.x  \n",
    "        taxi_zone['lat'] = taxi_zone.centroid.y\n",
    "        # to save time in the upcoming merging process, we remove unnecessary columns first\n",
    "        taxi_zone = taxi_zone[['LocationID','lon','lat']]\n",
    "        taxi_zone.drop_duplicates('LocationID',inplace=True)\n",
    "        taxi_zone.set_index('LocationID',inplace=True)\n",
    "        \n",
    "        # we apply the relation between location ID and longtitude and latitude to our real world dataframe\n",
    "        df['pickup_latitude'] = df['PULocationID'].map(taxi_zone['lat'])\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(taxi_zone['lon'])\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(taxi_zone['lat'])\n",
    "        df['dropoff_longitude'] = df['DOLocationID'].map(taxi_zone['lon'])\n",
    "        return df\n",
    "    \n",
    "    # not every dataset contains location ID, which needs to be interpreted. \n",
    "    # so we run a function to judge if a translation is needed\n",
    "    if 'PULocationID' in trip_data.columns:\n",
    "        trip_data = ID_to_LonLat(trip_data)\n",
    "    \n",
    " \n",
    "    \n",
    "    # normalizing column;\n",
    "    # rename columns\n",
    "    columns_rename = {\n",
    "        'tpep_pickup_datetime':'pickup_datetime',\n",
    "        'Trip_Pickup_DateTime':'pickup_datetime',\n",
    "        'Start_Lon':'pickup_longitude',\n",
    "        'lon_pickup':'pickup_longitude',\n",
    "        'Start_Lat':'pickup_latitude',\n",
    "        'lat_pickup':'pickup_latitude',\n",
    "        'lon_dropoff':'dropoff_longitude',\n",
    "        'End_Lon':'dropoff_longitude',\n",
    "        'lat_dropoff':'dropoff_latitude',\n",
    "        'End_Lat':'dropoff_latitude',\n",
    "        'Tip_Amt':'tip_amount'\n",
    "    }\n",
    "    trip_data = trip_data.rename(columns=columns_rename)\n",
    "    \n",
    "    # only keep date, time and geographic information, which we will use later (second time filtering)\n",
    "    trip_data = trip_data[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','tip_amount']]\n",
    "    \n",
    "    \n",
    "    # select valid data; \n",
    "    trip_data = trip_data[(trip_data['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (trip_data['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    trip_data = trip_data[(trip_data['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (trip_data['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    trip_data = trip_data[(trip_data['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (trip_data['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    trip_data = trip_data[(trip_data['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (trip_data['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])] \n",
    "    \n",
    "    # Separate the date and the specific time, then create a distinct column\n",
    "    trip_data ['pickup_datetime'] = trip_data['pickup_datetime'].map(lambda x: str(x)[:-3])\n",
    "    trip_data ['pickup_datetime']= pd.to_datetime(trip_data['pickup_datetime'], format=\"%Y-%m-%d %H:%M:%S\") \n",
    "    trip_data ['Date']= trip_data['pickup_datetime'].dt.date\n",
    "    trip_data ['Hour'] = trip_data['pickup_datetime'].dt.hour\n",
    "    trip_data = trip_data.drop('pickup_datetime',axis=1)\n",
    "\n",
    "    # Defining the data type of each column in the dataframe\n",
    "    trip_data['Date'] = pd.to_datetime(trip_data['Date'])\n",
    "    trip_data = trip_data.astype({'pickup_longitude':'float','pickup_latitude':'float','dropoff_longitude':'float','dropoff_latitude':'float','tip_amount':'float'})\n",
    "    \n",
    "    \n",
    "    return trip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {
    "id": "35c9c0cd"
   },
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    \"\"\"\n",
    "    Add distance column to each yellow taxi dataframe\n",
    "\n",
    "    The function will return a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    all_taxi_dataframes = []\n",
    "    # process all the url\n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    # add the distance column to every dataframe\n",
    "    for csv_url in all_csv_urls:\n",
    "\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        add_distance_column(dataframe)\n",
    "\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "    # concatenate every dataframe\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {
    "id": "094b4d6d"
   },
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "For this part, we will load and clean the uber data. We need to make sure the data type and size of uber consistent with yellow taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {
    "id": "7c58e3a2"
   },
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    load the uber data and clean uber data\n",
    "\n",
    "    Parameter Explanation:\n",
    "    csv_file ---- the file we need to process\n",
    "\n",
    "    The function will return a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the data\n",
    "    data = pd.read_csv(csv_file)\n",
    "    print(\"cleaning\", csv_file)\n",
    "    # Select valid data\n",
    "    data = data[(data['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (data['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    data = data[(data['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (data['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    data = data[(data['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (data['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    data = data[(data['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (data['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]    \n",
    "    UBER_DATA = data[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]\n",
    "\n",
    "    # Separate the date and the specific time, then create a distinct column\n",
    "    UBER_DATA ['pickup_datetime'] = UBER_DATA['pickup_datetime'].map(lambda x: str(x)[:-3])\n",
    "    UBER_DATA ['pickup_datetime']= pd.to_datetime(UBER_DATA['pickup_datetime'], format=\"%Y-%m-%d %H:%M:%S\") \n",
    "    UBER_DATA ['Date']= UBER_DATA['pickup_datetime'].dt.date\n",
    "    UBER_DATA ['Hour'] = UBER_DATA['pickup_datetime'].dt.hour\n",
    "    UBER_DATA = UBER_DATA.drop('pickup_datetime',axis=1)\n",
    "\n",
    "    # Defining the data type of each column in the dataframe\n",
    "    UBER_DATA['Date'] = pd.to_datetime(UBER_DATA['Date'])\n",
    "    UBER_DATA = UBER_DATA.astype({'pickup_longitude':'float','pickup_latitude':'float','dropoff_longitude':'float','dropoff_latitude':'float'})\n",
    "    return UBER_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {
    "id": "f836f118"
   },
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\"\n",
    "    Add distance column to uber dataframe\n",
    "\n",
    "    The function will return a dataframe\n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe = uber_dataframe.astype({'distance':'float'})\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {
    "id": "45a15cbb"
   },
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "For this part, we will load and clean the weather data in hourly and daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {
    "id": "76e864ab"
   },
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"\n",
    "    load the uber data and clean of each year of weather data in hourly\n",
    "\n",
    "    Parameter Explanation:\n",
    "    csv_file ---- the file we need to process\n",
    "\n",
    "    The function will return a dataframe\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    # Import the data and select valid data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    weather_hourly_data=df[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    # Remove missing values\n",
    "    weather_hourly_data['HourlyPrecipitation'] = pd.to_numeric(weather_hourly_data['HourlyPrecipitation'], errors='coerce')\n",
    "    # Calculate average speed in hourly\n",
    "    wind_speed_average = int(weather_hourly_data['HourlyWindSpeed'].mean())\n",
    "\n",
    "    # fill out the missing values\n",
    "    weather_hourly_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    weather_hourly_data['HourlyWindSpeed'].fillna(wind_speed_average, inplace=True)\n",
    "\n",
    "    # Separate the date and the specific time, then create a distinct column\n",
    "    weather_hourly_data ['DATE']= pd.to_datetime(weather_hourly_data['DATE'], format=\"%Y-%m-%d %H:%M:%S\") \n",
    "    weather_hourly_data ['Date']= weather_hourly_data['DATE'].dt.date\n",
    "    weather_hourly_data ['time']  = weather_hourly_data['DATE'].dt.hour\n",
    "    weather_hourly_data = weather_hourly_data.drop('DATE',axis=1)\n",
    "\n",
    "    # Defining the data type of each column in the dataframe\n",
    "    weather_hourly_data['Date'] = pd.to_datetime(weather_hourly_data['Date'])\n",
    "    weather_hourly_data = weather_hourly_data.astype({'HourlyPrecipitation':'float','HourlyWindSpeed':'int'})  \n",
    "    \n",
    "    return weather_hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {
    "id": "0687581f"
   },
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"\n",
    "    load the uber data and clean of each year of weather data in hourly and get the sunrise and sunset data.\n",
    "\n",
    "    Parameter Explanation:\n",
    "    csv_file ---- the file we need to process\n",
    "\n",
    "    The function will return two dataframes ---- weather daily and daily sunset/sunrise\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the data\n",
    "    weather_daily_data = pd.read_csv(csv_file)\n",
    "    weather_daily_data['DATE'] = pd.to_datetime(weather_daily_data['DATE'])\n",
    "    \n",
    "    #Remove missing values and fill out the missing values\n",
    "    weather_daily_data['HourlyPrecipitation'] = pd.to_numeric(weather_daily_data['HourlyPrecipitation'], errors='coerce')\n",
    "    weather_daily_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "\n",
    "    #calculate the average windspeed and sum of the precipitation\n",
    "    weather_daily_data['DATE'] = weather_daily_data['DATE'].dt.date\n",
    "    weather_daily_data_1 = weather_daily_data.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': np.mean, 'HourlyPrecipitation': np.sum})\n",
    "    weather_daily_data_1 = weather_daily_data_1[['DATE','HourlyWindSpeed','HourlyPrecipitation']]\n",
    "    # Defining the data type of each column in the dataframe\n",
    "    weather_daily_data_1['DATE'] = pd.to_datetime(weather_daily_data_1['DATE'])\n",
    "    weather_daily_data_1.rename(columns={'DATE':'Date','HourlyWindSpeed': 'DailyWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    weather_daily_data_1 = weather_daily_data_1.astype({'DailyWindSpeed':'float', 'DailyPrecipitation':'float'})\n",
    "    \n",
    "    #Get the data of sunrise and sunset\n",
    "    weather_daily_data_sun= weather_daily_data.groupby('DATE', as_index=False).agg({'Sunrise': 'first', 'Sunset': 'first'})\n",
    "    weather_daily_data_sun = weather_daily_data[['DATE','Sunrise','Sunset']]\n",
    "    weather_daily_data_sun = weather_daily_data_sun.dropna()\n",
    "    \n",
    "    # Defining the data type of each column in the dataframe\n",
    "    weather_daily_data_sun.rename(columns={'DATE':'Date'}, inplace=True)\n",
    "    weather_daily_data_sun['Date'] = pd.to_datetime(weather_daily_data_sun['Date'])\n",
    "    weather_daily_data_sun = weather_daily_data_sun.astype({'Sunrise': 'int', 'Sunset': 'int'})\n",
    "    \n",
    "    \n",
    "    return weather_daily_data_1, weather_daily_data_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {
    "id": "3ef8945d"
   },
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"\n",
    "    Process all the weather data we need. \n",
    "\n",
    "    The function will return a dataframe\n",
    "    \"\"\"\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    sun_dataframes = []\n",
    "    \n",
    "    # all the name of the weather files\n",
    "    weather_csv_files = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\", \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        print(\"cleaning\", csv_file)\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)[0]\n",
    "        daily_sun_dataframe = clean_month_weather_data_daily(csv_file)[1]\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        sun_dataframes.append(daily_sun_dataframe)\n",
    "        \n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    sun_data = pd.concat(sun_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data, sun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {
    "id": "f900f7aa"
   },
   "source": [
    "### Process All Data\n",
    "\n",
    "Now, we can get all the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {
    "id": "f7cd53a6"
   },
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data, sun_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin unit test, to ensure we have the right columns and the column of 'distance' added\n",
    "def test_add_col(df):\n",
    "    # expected column names\n",
    "    columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "       'dropoff_latitude', 'tip_amount', 'Date', 'Hour', 'distance']\n",
    "    # ensure every required name, including 'distance' is in the dataframe\n",
    "    for column in columns:\n",
    "        assert column in df.columns\n",
    "    \n",
    "## Begin test  \n",
    "test_add_col(taxi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {
    "id": "dd101f11"
   },
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "We need to create a database for storing cleaned data by SQL and sqlalchemy. We mainly will do the following steps:\n",
    "\n",
    "\n",
    "1.   Build the table for each dataframe.\n",
    "2.   Create schema.sql file \n",
    "3.   Save all the table in the schema.sql file\n",
    "4.   Add all the data into the database\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {
    "id": "f3529cf6"
   },
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {
    "id": "d2bea0ff"
   },
   "outputs": [],
   "source": [
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather_schema\n",
    "(\n",
    "  Date DATE,\n",
    "  time TIME,\n",
    "  HourlyPrecipitation INTEGER,\n",
    "  HourlyWindGustSpeed INTEGER,\n",
    "  id INTEGER PRIMARY KEY\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather_schema\n",
    "(\n",
    "  Date DATE,\n",
    "  DailyPrecipitation INTEGER,\n",
    "  DailyAverageWindSpeed INTEGER,\n",
    "  id INTEGER PRIMARY KEY\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips    \n",
    "(\n",
    "  Date DATE,\n",
    "  Hour TIME,\n",
    "  pickup_longitude FLOAT,\n",
    "  pickup_latitude FLOAT,\n",
    "  dropoff_longitude FLOAT,\n",
    "  dropoff_latitude FLOAT,\n",
    "  distance FLOAT,\n",
    "  tip_amount FLOAT,\n",
    "  id INTEGER PRIMARY KEY\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather_schema\n",
    "(\n",
    "  Date DATE,\n",
    "  Hour TIME,\n",
    "  pickup_longitude FLOAT,\n",
    "  pickup_latitude FLOAT,\n",
    "  dropoff_longitude FLOAT,\n",
    "  dropoff_latitude FLOAT,\n",
    "  distance FLOAT,\n",
    "  id INTEGER PRIMARY KEY\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "SUN_DATA_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sun_data \n",
    "(\n",
    "    Date DATE,\n",
    "    Sunrise INT,\n",
    "    Sunset INT,\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {
    "id": "5f41e54b"
   },
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(SUN_DATA_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {
    "id": "02eccdba"
   },
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    with open(DATABASE_SCHEMA_FILE, \"r\") as f:\n",
    "        rows = f.readlines()\n",
    "        extra_space = []\n",
    "        for line in rows:\n",
    "            extra_space.append(line)\n",
    "            # if the line is a ')', execute the query\n",
    "            if \")\" in line:\n",
    "                connection.execute(db.text(\"\".join(extra_space)))\n",
    "                extra_space = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {
    "id": "c122964f"
   },
   "source": [
    "### Add Data to Database\n",
    "\n",
    "Add all the cleaned data into the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {
    "id": "0e68a363"
   },
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for table, dataframe in table_to_df_dict.items():\n",
    "        print(\"writing\", table)\n",
    "        dataframe.to_sql(table, con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {
    "id": "45d6c06c"
   },
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "    \"sun_data\": sun_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {
    "id": "74004f96"
   },
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {
    "id": "8cb6e33e"
   },
   "source": [
    "## Part 3: Understanding the Data\n",
    "\n",
    "We will be crafting a set of SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {
    "id": "6a849e92"
   },
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {
    "id": "ee70a777"
   },
   "source": [
    "### Query 1\n",
    "\n",
    "For 01-2009 through 06-2015, the most popular hour of the day to take a yellow taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {
    "id": "db871d3c"
   },
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "Select Hour As hours, Count(*) As amount\n",
    "From taxi_trips\n",
    "Group By hours\n",
    "Order By amount DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PAjVOaGkCIKN",
   "metadata": {
    "id": "PAjVOaGkCIKN"
   },
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {
    "id": "a2ef04df"
   },
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"most_popular_hour.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rN1riP3j8qNf",
   "metadata": {
    "id": "rN1riP3j8qNf"
   },
   "source": [
    "### Query 2\n",
    "\n",
    "Find the most popular day of the week to take a uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vj7fc0E7YmA7",
   "metadata": {
    "id": "vj7fc0E7YmA7"
   },
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "Select strftime('%w', Date) As weekday, Count(*) As amount\n",
    "From uber_trips\n",
    "Group By weekday\n",
    "Order By amount DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6M33E2NQYnAa",
   "metadata": {
    "id": "6M33E2NQYnAa"
   },
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MbH7M8JbYoJD",
   "metadata": {
    "id": "MbH7M8JbYoJD"
   },
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"most_popular_day.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zbB2twqk8skA",
   "metadata": {
    "id": "zbB2twqk8skA"
   },
   "source": [
    "### Query 3\n",
    "\n",
    "Find the 95% percentile of distance traveled for all hired trips during July 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LHmSM9cb8dVB",
   "metadata": {
    "id": "LHmSM9cb8dVB"
   },
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "WITH hired_trips AS\n",
    "(\n",
    "SELECT Date, distance FROM taxi_trips WHERE Date BETWEEN '2013-07-01' AND '2013-08-01'\n",
    "UNION ALL\n",
    "SELECT Date, distance FROM uber_trips WHERE Date BETWEEN '2013-07-01' AND '2013-08-01'\n",
    ")\n",
    "\n",
    "SELECT distance AS '95%_percentile_of_distance'\n",
    "FROM hired_trips\n",
    "ORDER BY distance ASC\n",
    "LIMIT 1\n",
    "OFFSET (SELECT COUNT(*) FROM hired_trips) * 95 / 100 - 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zU_qQs8H8dKQ",
   "metadata": {
    "id": "zU_qQs8H8dKQ"
   },
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhLpKZFV8hUF",
   "metadata": {
    "id": "xhLpKZFV8hUF"
   },
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95%_percentile_of_distance_201307.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_VAgOgCL84UO",
   "metadata": {
    "id": "_VAgOgCL84UO"
   },
   "source": [
    "### Query 4\n",
    "\n",
    "Find the top 10 days with the highest number of hired rides for 2009, and the average distance for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zygc0M9L8iOA",
   "metadata": {
    "id": "Zygc0M9L8iOA"
   },
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "WITH hired_trips AS\n",
    "(\n",
    "SELECT Date, distance FROM taxi_trips WHERE Date BETWEEN '2009-01-01' AND '2009-12-31'\n",
    "UNION ALL\n",
    "SELECT Date, distance FROM uber_trips WHERE Date BETWEEN '2009-01-01' AND '2009-12-31'\n",
    ")\n",
    "\n",
    "\n",
    "SELECT DATE(Date), COUNT(*) AS 'number_of_hired_rides', distance / COUNT(*) AS 'daily_average_distance'\n",
    "FROM hired_trips \n",
    "GROUP BY DATE(Date)\n",
    "ORDER BY number_of_hired_rides DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BzKk2DcI8iHs",
   "metadata": {
    "id": "BzKk2DcI8iHs"
   },
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RyrRAHTdFF7d",
   "metadata": {
    "id": "RyrRAHTdFF7d"
   },
   "source": [
    "### Query 5\n",
    "\n",
    "Find the number of hired trips on the top 10 windiest days in 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ji2Xo88RFMud",
   "metadata": {
    "id": "Ji2Xo88RFMud"
   },
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "WITH hired_trips(date) AS\n",
    "(\n",
    "SELECT DATE(Date) FROM taxi_trips WHERE Date BETWEEN '2014-01-01' AND '2014-12-31'\n",
    "UNION ALL\n",
    "SELECT DATE(Date) FROM uber_trips WHERE Date BETWEEN '2014-01-01' AND '2014-12-31'\n",
    ")\n",
    "\n",
    "SELECT date , COUNT(*) AS 'trips' FROM hired_trips\n",
    "GROUP BY date\n",
    "HAVING date IN \n",
    "(\n",
    "SELECT DATE(Date) \n",
    "FROM daily_weather \n",
    "WHERE DATE(Date) BETWEEN '2014-01-01' AND '2014-12-31' \n",
    "ORDER BY DailyWindSpeed DESC \n",
    "LIMIT 10\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ujm0qAEgFM2L",
   "metadata": {
    "id": "Ujm0qAEgFM2L"
   },
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56397a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"10_windiest_days_in_2014.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZDD-vcFLFNXk",
   "metadata": {
    "id": "ZDD-vcFLFNXk"
   },
   "source": [
    "### Query 6\n",
    "\n",
    "Find out how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what the sustained wind speed was during Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WaKV15o5Fqsk",
   "metadata": {
    "id": "WaKV15o5Fqsk"
   },
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "WITH hired_trips(date) AS\n",
    "(\n",
    "SELECT DATE(Date) FROM taxi_trips WHERE Date BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "UNION ALL\n",
    "SELECT DATE(Date) FROM uber_trips WHERE Date BETWEEN '2012-10-22' AND '2012-11-06'\n",
    ")\n",
    "\n",
    "SELECT Date(hourly_weather.Date) AS weather_date, COUNT(hired_trips.DATE) AS 'number_of_days', HourlyPrecipitation, HourlyWindSpeed\n",
    "FROM hourly_weather\n",
    "LEFT JOIN hired_trips\n",
    "ON weather_date = hired_trips.date\n",
    "WHERE weather_date BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "GROUP BY weather_date;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qmf7O7NiFqxU",
   "metadata": {
    "id": "Qmf7O7NiFqxU"
   },
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9nSHVttsy9Jd",
   "metadata": {
    "id": "9nSHVttsy9Jd"
   },
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"hurricane_trips.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {
    "id": "a13ced42"
   },
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "Creating visualizations to enhance the understanding of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IiTGJC4RubA4",
   "metadata": {
    "id": "IiTGJC4RubA4"
   },
   "source": [
    "### **Visualization 1**\n",
    "\n",
    "Create an appropriate visualization for the most popular hour of the day to take a yellow taxi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {
    "id": "0de8394c"
   },
   "outputs": [],
   "source": [
    "def hour_for_taxi_trip(dataframe):\n",
    "    # The format of the diagram \n",
    "    dataframe.plot(x=\"hours\", y=\"amount\",kind=\"bar\",figsize=(10, 10))\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Number of Trips')\n",
    "    plt.title('Num of Taxi Trips per Hour')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {
    "id": "847ced2f"
   },
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # select the data  \n",
    "    df = pd.read_sql_query(QUERY_1, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {
    "id": "3c63e845"
   },
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "hour_for_taxi_trip(some_dataframe.sort_values(by=\"hours\",ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UdW42podu-CQ",
   "metadata": {
    "id": "UdW42podu-CQ"
   },
   "source": [
    "### **Visualization 2**\n",
    "\n",
    "Plot an histogram to show the average the average distance traveled per month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Id5qX0yZvmNA",
   "metadata": {
    "id": "Id5qX0yZvmNA"
   },
   "outputs": [],
   "source": [
    "def average_distance_traveled(dataframe):\n",
    "    # The format of the diagram \n",
    "    dataframe.plot(x=\"month\", y=\"average\",kind=\"bar\",figsize=(10, 10))\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average distance')\n",
    "    plt.title('Average Distance Traveled Per Month')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sBE4VsQ3vtR3",
   "metadata": {
    "id": "sBE4VsQ3vtR3"
   },
   "outputs": [],
   "source": [
    "def get_data_for_visual_2():\n",
    "\n",
    "    # select the data  \n",
    "    query_average_distance = '''\n",
    "    WITH hired_trips AS \n",
    "    (\n",
    "    SELECT Date,distance FROM taxi_trips \n",
    "    UNION ALL\n",
    "    SELECT Date,distance FROM uber_trips\n",
    "    )\n",
    "\n",
    "    SELECT strftime('%m', Date) as month, AVG(average_month_distance) as average\n",
    "    From (SELECT Date, SUM(distance) AS average_month_distance FROM hired_trips \n",
    "    group by strftime('%Y-%m', Date))\n",
    "    Group by month;\n",
    "    '''\n",
    "    df = pd.read_sql_query(query_average_distance, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_YTEdW8SvvK_",
   "metadata": {
    "id": "_YTEdW8SvvK_"
   },
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_2()\n",
    "average_distance_traveled(some_dataframe.sort_values(by=\"month\", ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61rsRaPwvz83",
   "metadata": {
    "id": "61rsRaPwvz83"
   },
   "source": [
    "### **Visualization 3**\n",
    "\n",
    "Plot a diagram show the most popular for drop offs for each airport (LGA, JFK, and EWR) day of the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qKwe5mizzDNj",
   "metadata": {
    "id": "qKwe5mizzDNj"
   },
   "outputs": [],
   "source": [
    "def dropoff_num_for_each_day(df):\n",
    "    df[0]['JFK'] = df[1]['JFK']\n",
    "    df[0]['EWR'] = df[2]['EWR']\n",
    "    plot = df[0].plot( x=\"day\", y=[\"LGA\",'JFK','EWR'], xlabel=\"Days\", ylabel=\"Number of Dropoffs\", kind=\"bar\", title=\"Number of Dropoffs of Each Day for each Airport\")\n",
    "    plot.set_xticklabels(('Sunday','Monday','Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htBdD9WZzDJl",
   "metadata": {
    "id": "htBdD9WZzDJl"
   },
   "outputs": [],
   "source": [
    "def get_data_for_visual_3():\n",
    "    # select the data \n",
    "    JFK_BBOX = [-73.826754,40.638476,-73.733714,40.667128,'JFK']\n",
    "    EWR_BBOX = [-74.200917,40.663963,-74.146328,40.709262,'EWR']\n",
    "    LGA_BBOX = [-73.891110,40.763483,-73.857121,40.785583,'LGA']\n",
    "    \n",
    "    \n",
    "    df = []\n",
    "    for BBOX in [LGA_BBOX, JFK_BBOX, EWR_BBOX]:\n",
    "        query = f'''\n",
    "        WITH hired_trips AS \n",
    "        (\n",
    "        SELECT Date, dropoff_longitude, dropoff_latitude FROM taxi_trips\n",
    "        UNION ALL\n",
    "        SELECT Date, dropoff_longitude,dropoff_latitude FROM uber_trips\n",
    "        )\n",
    "        \n",
    "        SELECT strftime('%w', Date) AS day, COUNT(*) AS {BBOX[4]}\n",
    "        FROM hired_trips\n",
    "        WHERE dropoff_longitude BETWEEN {BBOX[0]} AND {BBOX[2]} AND dropoff_latitude BETWEEN {BBOX[1]} AND {BBOX[3]}\n",
    "        GROUP BY day\n",
    "        '''\n",
    "        df.append(pd.read_sql_query(query, engine))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gaKOVSDJzDE9",
   "metadata": {
    "id": "gaKOVSDJzDE9"
   },
   "outputs": [],
   "source": [
    "airport_dataframe = get_data_for_visual_3()\n",
    "dropoff_num_for_each_day(airport_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HpDN01wJv7Lm",
   "metadata": {
    "id": "HpDN01wJv7Lm"
   },
   "source": [
    "### **Visualization 4**\n",
    "Create a heatmap of all hired trips over a map of the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dnoiQTCjzjpF",
   "metadata": {
    "id": "dnoiQTCjzjpF"
   },
   "outputs": [],
   "source": [
    "def heatmap_of_hired_trips(df):\n",
    "    # The format of the diagram\n",
    "    def generateBaseMap(default_location=[40.693943, -73.985880], default_zoom_start=11):\n",
    "        base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
    "        return base_map\n",
    "\n",
    "    base_map = generateBaseMap()\n",
    "    HeatMap(data=df[['pickup_latitude', 'pickup_longitude', 'count']].\\\n",
    "            groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist(), \\\n",
    "            radius=8, max_zoom=13).add_to(base_map)\n",
    "    \n",
    "    return base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1GedR6o3zja7",
   "metadata": {
    "id": "1GedR6o3zja7"
   },
   "outputs": [],
   "source": [
    "def get_data_for_visual_4():\n",
    "    # select the data as dataframe\n",
    "    taxi_data_copy = taxi_data.copy()\n",
    "    taxi_data_copy['count'] = 1\n",
    "    taxi_data_copy[['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().sort_values('count', ascending=False).head(10)\n",
    "    \n",
    "    return taxi_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfAJ1mtzjWt",
   "metadata": {
    "id": "1bfAJ1mtzjWt"
   },
   "outputs": [],
   "source": [
    "hired_trips_dataframe = get_data_for_visual_4()\n",
    "heatmap_of_hired_trips(hired_trips_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JNV74r3Sv7SW",
   "metadata": {
    "id": "JNV74r3Sv7SW"
   },
   "source": [
    "### **Visualization 5**\n",
    "\n",
    "Plot a scatter to show the relatonship between tip amount and distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5iA1DJPHwQ-3",
   "metadata": {
    "id": "5iA1DJPHwQ-3"
   },
   "outputs": [],
   "source": [
    "def tip_amount(dataframe):\n",
    "    # The format of the diagram \n",
    "    dataframe.plot(x=\"distance\", y=\"tip_amount\",kind=\"scatter\",figsize=(10, 10))\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Tip')\n",
    "    plt.title('Distance VS Tip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "woTGwe0xwjmP",
   "metadata": {
    "id": "woTGwe0xwjmP"
   },
   "outputs": [],
   "source": [
    "def get_data_for_visual_5():\n",
    "    \n",
    "    # select the data    \n",
    "    query_tip = '''\n",
    "    SELECT tip_amount, distance FROM taxi_trips\n",
    "    '''\n",
    "    df = pd.read_sql_query(query_tip, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3LEWGtCzwluA",
   "metadata": {
    "id": "3LEWGtCzwluA"
   },
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_5()\n",
    "tip_amount(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sUp7a4Ziwnw4",
   "metadata": {
    "id": "sUp7a4Ziwnw4"
   },
   "source": [
    "### **Visualization 6**\n",
    "\n",
    "Plot a scatter to show the relatonship between tip amount and precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hr3gq0lQwyOg",
   "metadata": {
    "id": "hr3gq0lQwyOg"
   },
   "outputs": [],
   "source": [
    "def tip_vs_precipitation(dataframe):\n",
    "    # The format of the diagram \n",
    "    dataframe.plot(x=\"precipitation\", y=\"tip\", kind=\"scatter\",figsize=(10, 10) )\n",
    "    plt.xlabel('Precipitation')\n",
    "    plt.ylabel('Tip Amount')\n",
    "    plt.title('Tip Amount vs Precipitation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iILHSMHpw0sv",
   "metadata": {
    "id": "iILHSMHpw0sv"
   },
   "outputs": [],
   "source": [
    "def get_data_for_visual_6():\n",
    "    \n",
    "    # select the data\n",
    "    query_precipitation = '''\n",
    "    Select tips.DATE, tips.tip_amount as tip, Daily.precipitation as precipitation\n",
    "        From(Select date(Date) AS DATE, SUM(taxi_trips.tip_amount) AS tip_amount\n",
    "             From taxi_trips\n",
    "             GROUP BY DATE) as tips,\n",
    "            (Select date(Date) AS DATE, DailyPrecipitation AS precipitation\n",
    "             FROM daily_weather) as Daily\n",
    "        Where tips.DATE = Daily.DATE\n",
    "    '''\n",
    "\n",
    "\n",
    "    df = pd.read_sql_query(query_precipitation, engine)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sUKRvWKWw2K3",
   "metadata": {
    "id": "sUKRvWKWw2K3"
   },
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_6()\n",
    "tip_vs_precipitation(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rN1riP3j8qNf"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
